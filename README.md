# Orders Dataset Analysis with PostgreSQL and Jupyter Notebook

## Overview
This project demonstrates the analysis of the `orders.csv` dataset. The workflow involves:
1. Connecting Jupyter Notebook to a PostgreSQL database.
2. Uploading and querying the dataset in PostgreSQL.
3. Extracting valuable business insights through SQL queries.

---

## Key Steps

### 1. Dataset Overview
- **Dataset Name**: `orders.csv`
- **Some of the Columns from dataset**:
  - `order_date`: Date when the order was placed.
  - `sales_price`: Revenue generated by the sale.
  - `category`, `sub_category`: Product categorization.
  - `region`: Geographical region of the sale.
  - `product_id`: Unique identifier for each product.

### 2. Connecting Jupyter Notebook to PostgreSQL
Used SQLAlchemy to establish a connection between Jupyter Notebook and PostgreSQL for seamless query execution.

**Python Code**:
```python
import sqlalchemy as sal

# Create engine and connect to PostgreSQL
engine = sal.create_engine('postgresql://username:password@localhost:5432/database_name')
conn = engine.connect()
